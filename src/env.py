from config import GameSettings
from random import random
from enum import Enum
import torch

from entities.obstacle import Obstacle
from entities.goal import Goal

class Action(Enum):
    UP    = (1, 0)
    DOWN  = (-1, 0)
    LEFT  = (0, -1)
    RIGHT = (0, 1)
    STAY  = (0, 0)

    def __call__(self):
        return torch.tensor(self.value, dtype=torch.float32)

class Env:

    def __init__(self, config: GameSettings):
        """
        Initializes the environment.
        The environment is a 2D grid where an agent can move in four directions
        (up, down, left, right) and can also stay in place.
        The agent has a speed of 1 unit per step and can interact with obstacles
        and a goal.
        
        Parameters:
        - config: Configuration settings for the environment
        
        Returns:
        None
        """
        self.config = config
        
        self.agent_pos = None
        self.goal = None
        self.obstacles = []        
        
        self.reset()  # Initialize the environment by resetting it


    def step(self, action: Action):
        """
        Applies an action to the environment.

        Parameters:
        - action: The action to be taken

        Return:
        tuple[state, reward, terminated] 
        - state: The updated observed state
        - reward: The reward generated by the action
        - terminated: True if the environment has reached its terminal state
        """
        initial_state = self.agent_pos.clone()  # Store the initial agent position

        self.agent_pos += self.config.agent_speed * action()

        # Check if the agent is out of bounds 
        if (self.agent_pos[0] < 0 or self.agent_pos[0] > self.config.screen_width or
            self.agent_pos[1] < 0 or self.agent_pos[1] > self.config.screen_height):
            self.agent_pos = initial_state
            reward = -1
            observation = torch.cat([
                self.agent_pos,
                self.goal.get_center(),
                *[obstacle.get_bounding_box() for obstacle in self.obstacles]
            ])
            return observation, reward, False

        # If this causes a collision, then undo and penalize
        for obstacle in self.obstacles:
            if obstacle.collides(self.agent_pos):
                self.agent_pos = initial_state
                reward = -1
                terminated = False
                break
        else:
            reward = 0
            # Check if the agent has reached the goal
            if self.goal.collides(self.agent_pos):
                reward = 10
                terminated = True
            else:
                # Small penalty for each time step
                reward = -0.01
                terminated = False

        # Return the new state, reward, and termination status
        observation = torch.cat([
            self.agent_pos,
            self.goal.get_center(),
            *[obstacle.get_bounding_box() for obstacle in self.obstacles]
        ])
        return observation, reward, terminated


    def reset(self, seed: int = 1738):
        """
        Resets the environment to an initial state. 

        Parameters:
        - seed: Seed for randomization (optional)

        Returns:
        - observation: Initial observation of new environment
        """
        # torch.manual_seed(seed)
       
        agent_x = torch.rand(1).item() * self.config.screen_width
        agent_y = torch.rand(1).item() * self.config.screen_height
        self.agent_pos = torch.tensor([agent_x, agent_y], dtype=torch.float32) 
       
        goal_x = torch.rand(1).item() * self.config.screen_width
        goal_y = torch.rand(1).item() * self.config.screen_height
        self.goal = Goal(goal_x, goal_y)
        # Ensure the goal is not too close to the agent
        while self.goal.collides(self.agent_pos):
            goal_x = torch.rand(1).item() * self.config.screen_width
            goal_y = torch.rand(1).item() * self.config.screen_height
            self.goal = Goal(goal_x, goal_y) 
        
        self.obstacles = [self.add_obstacle() for _ in range(self.config.num_obstacles)]

        # Return the initial observation
        return torch.cat([
            self.agent_pos,
            self.goal.get_center(),
            *[obstacle.get_bounding_box() for obstacle in self.obstacles]
        ])

    def close(self):
        """
        Cleans up the environment

        Parameters:
        None

        Returns:
        None
        """
        ...

    @property
    def action_space(self):
        """
        Returns the possible actions from the current state
        """
        return list(Action)

    @property 
    def observation_space(self):
        """
        Returns the space of possible observations

        Observations include agent position, goal position, and
        bounding boxes of all obstacles.

        [
            agent_x, agent_y,                        # Coordinates of agent
            goal_x, goal_y,                          # Coordinates of goal
            obs1_x1, obs1_y1, obs1_x2, obs1_y2,      # Bounding box of obstacle 1
            obs2_x1, obs2_y1, obs2_x2, obs2_y2,      # Bounding box of obstacle 2
            ...                                      # Additional obstacles
        ]
        """
        num_obstacles = self.config.num_obstacles
        return [
            "agent_x", "agent_y",
            "goal_x", "goal_y",
            *[(f"obs{i}_x1", f"obs{i}_y1", f"obs{i}_x2", f"obs{i}_y2") for i in range(1, num_obstacles + 1)]
        ]

    def add_obstacle(self):
        """
        Adds a random obstacle to the environment.

        Parameters:
        None

        Returns:
        None
        """
       
        max_attempts = 100  # Limit the number of attempts to avoid infinite loops
        for _ in range(max_attempts):
            obstacle = Obstacle.random(self.config.screen_width, self.config.screen_height)
            # Check if the new obstacle collides with any existing entity
            if all(
                not existing_obstacle.collides(obstacle)
                for existing_obstacle in self.obstacles
            ) and not self.goal.collides(obstacle) and not obstacle.collides(self.agent_pos):
                return obstacle  # Return the valid obstacle

        raise RuntimeError("Failed to generate a valid obstacle after multiple attempts.")
